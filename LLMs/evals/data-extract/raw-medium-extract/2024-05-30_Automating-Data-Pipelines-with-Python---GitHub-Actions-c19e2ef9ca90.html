<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Automating Data Pipelines with Python &amp; GitHub Actions</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Automating Data Pipelines with Python &amp; GitHub Actions</h1>
</header>
<section data-field="subtitle" class="p-summary">
A simple (and free) way to run data workflows
</section>
<section data-field="body" class="e-content">
<section name="8f88" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1f8f" id="1f8f" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Automating Data Pipelines with Python &amp; GitHub¬†Actions</strong></h3><h4 name="036a" id="036a" class="graf graf--h4 graf-after--h3 graf--subtitle">A simple (and free) way to run data workflows</h4><p name="4d87" id="4d87" class="graf graf--p graf-after--h4">This is the 4th article in a <a href="https://shawhin.medium.com/list/full-stack-data-science-f0910c75d006" data-href="https://shawhin.medium.com/list/full-stack-data-science-f0910c75d006" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on Full Stack Data Science (FSDS). In the <a href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" data-href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">last post</a>, I shared a concrete example of how to build data pipelines for machine learning projects. One limitation of the example, however, was that the data pipeline had to be run manually. While this might be fine for some applications, more often than not, it‚Äôs better to automate than delegate this process to a person. In this article, I will walk through a simple way to do this using Python and GitHub Actions.</p><figure name="ef4b" id="ef4b" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*rrbrX7Jv6VlDdkNE" data-width="5876" data-height="3314" data-unsplash-photo-id="jL6PTWI7h18" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*rrbrX7Jv6VlDdkNE"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@chenhanozel?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@chenhanozel?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Chen Mizrach</a> on¬†<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="a206" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="15ee" id="15ee" class="graf graf--p graf--leading">A few years ago, Andrej Karpathy gave a talk describing ‚ÄúOperation Vacation‚Äù [1]. This was what Tesla‚Äôs full self-driving engineering team called their goal of completely automating the improvement of the self-driving model.</p><p name="1e72" id="1e72" class="graf graf--p graf-after--p">Although this goal was somewhat facetious, it illustrates an aspiration I‚Äôve seen in most data scientists and engineers: <strong class="markup--strong markup--p-strong">the desire to build a system that can operate autonomously</strong> (so they can go on vacation).</p><p name="e13f" id="e13f" class="graf graf--p graf-after--p">Here, I will discuss how to automate a key element of any machine learning system‚Äîthe data pipeline.</p><figure name="048d" id="048d" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/wJ794jLP2Tw?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="b32d" id="b32d" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">2 Ways to Automate Data Pipelines</strong></h3><p name="7537" id="7537" class="graf graf--p graf-after--h3">While there are countless ways to build and automate data pipelines, here I‚Äôll categorize the approaches into two buckets: using an orchestration tool and not using an orchestration tool.</p><h3 name="5507" id="5507" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Way 1: Orchestration Tool</strong></h3><p name="6dc7" id="6dc7" class="graf graf--p graf-after--h3">Orchestration tools allow developers to manage workflows with hundreds (and even thousands) steps.</p><h4 name="306d" id="306d" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Airflow</strong></h4><p name="ea84" id="ea84" class="graf graf--p graf-after--h4">One of the most popular orchestration tools is Airflow, which can manage complex workflows using Python. This has made it a standard among data engineers managing enterprise data pipelines.</p><p name="a1bd" id="a1bd" class="graf graf--p graf-after--p">A downside of Airflow, however, is that its setup and maintenance can be complicated. Consequently, it requires a strong technical understanding of how it works, which can take time to develop.</p><h4 name="af4a" id="af4a" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Airflow Wrappers</strong></h4><p name="9ab0" id="9ab0" class="graf graf--p graf-after--h4">The complexity of Airflow has led to the rise of Airflow wrappers, which make its core functionality easier to use. Popular examples of these tools include <a href="https://docs.dagster.io/getting-started" data-href="https://docs.dagster.io/getting-started" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dagster</a>, <a href="https://docs.mage.ai/introduction/overview" data-href="https://docs.mage.ai/introduction/overview" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mage</a>, <a href="https://www.astronomer.io/docs/" data-href="https://www.astronomer.io/docs/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Astronomer</a>, and <a href="https://www.prefect.io/opensource" data-href="https://www.prefect.io/opensource" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Prefect</a> (<em class="markup--em markup--p-em">not a wrapper, but comparable</em>).</p><p name="478d" id="478d" class="graf graf--p graf-after--p">While all these tools provide scalable and reliable ways to manage sophisticated data workflows, <strong class="markup--strong markup--p-strong">they might be overkill for many ML applications</strong>. So, let‚Äôs take a step back and ask how we can build pipelines without orchestration tools.</p><h3 name="99c3" id="99c3" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Way 2: Python +¬†Triggers</strong></h3><p name="fb28" id="fb28" class="graf graf--p graf-after--h3">As discussed in the <a href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" data-href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a>, data pipelines consist of <strong class="markup--strong markup--p-strong">three basic tasks: extraction, transformation, and loading</strong>. All of these can be implemented using Python.</p><p name="3baf" id="3baf" class="graf graf--p graf-after--p">For example, if we wanted to pull data from a single data source and store it in a single database, the workflow would look like this.</p><figure name="4f26" id="4f26" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*s-qAvBPOBXaYFZLPIUHfCQ.png" data-width="1131" data-height="263" src="https://cdn-images-1.medium.com/max/800/1*s-qAvBPOBXaYFZLPIUHfCQ.png"><figcaption class="imageCaption">A simple data pipeline. Image by¬†author.</figcaption></figure><p name="d875" id="d875" class="graf graf--p graf-after--figure">While we could surely use an orchestration tool for this, nothing stops us from consolidating the ETL scripts into a single Python file and running that.</p><figure name="4531" id="4531" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HyhIF4jUCamsU4Yikv2CUw.png" data-width="634" data-height="230" src="https://cdn-images-1.medium.com/max/800/1*HyhIF4jUCamsU4Yikv2CUw.png"><figcaption class="imageCaption">Consolidated data pipeline. Image by¬†author.</figcaption></figure><h4 name="ac58" id="ac58" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Automating with¬†Triggers</strong></h4><p name="f3b5" id="f3b5" class="graf graf--p graf-after--h4">The above abstraction simplifies the execution of this pipeline, but it‚Äôs still not automated since we manually have to run the <em class="markup--em markup--p-em">etl.py</em> script. To take the next step, we need to introduce a trigger.</p><p name="140f" id="140f" class="graf graf--p graf-after--p">A <strong class="markup--strong markup--p-strong">trigger</strong> runs a command when a specific criterion is satisfied. For example, the time is 12:00 AM, or a new file appears in a directory.</p><p name="ca88" id="ca88" class="graf graf--p graf-after--p">This final piece allows us to fully automate the data pipeline so we can spend our time doing more productive things e.g. read Medium articles üòâ.</p><p name="5e20" id="5e20" class="graf graf--p graf-after--p">Here‚Äôs what our example pipeline would look like if running every day at midnight.</p><figure name="fe74" id="fe74" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NFNUDbl9frRFzSfufJqWAA.png" data-width="775" data-height="302" src="https://cdn-images-1.medium.com/max/800/1*NFNUDbl9frRFzSfufJqWAA.png"><figcaption class="imageCaption">Consolidated data pipeline running every day at midnight via cron. Image by¬†author.</figcaption></figure><h3 name="6cbc" id="6cbc" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">GitHub Actions</strong></h3><p name="50c0" id="50c0" class="graf graf--p graf-after--h3">We can implement the simple workflow described above via <strong class="markup--strong markup--p-strong">GitHub Actions (GA)</strong>. GA is a <strong class="markup--strong markup--p-strong">CI/CD (continuous integration, continuous delivery) platform</strong>, which is a fancy way of saying it helps you automate software testing and updating.</p><p name="0723" id="0723" class="graf graf--p graf-after--p">While data may traditionally be seen to sit outside of software, these are inseparable when it comes to machine learning. This is because <strong class="markup--strong markup--p-strong">ML uses data to ‚Äúwrite‚Äù software</strong> (i.e. train models).</p><p name="5c5f" id="5c5f" class="graf graf--p graf-after--p">The biggest upside of using GA is GitHub provides free computing to run actions for public repositories, which is great for poor developers (like me) and simple proof-of-concept projects.</p><h3 name="f234" id="f234" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example Code: Automating ETL Pipeline for YouTube Video Transcripts</strong></h3><p name="f4ac" id="f4ac" class="graf graf--p graf-after--h3">Let‚Äôs walk through a concrete example of automating a simple ETL people using GitHub Actions. I‚Äôll do this for the example code from the <a href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" data-href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> in this series, where I extracted transcripts from all the videos from my YouTube channel.</p><p name="6f2b" id="6f2b" class="graf graf--p graf-after--p graf--trailing">Example code is freely available at the <a href="https://github.com/ShawhinT/data-pipeline-example" data-href="https://github.com/ShawhinT/data-pipeline-example" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p></div></div></section><section name="7d20" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="677f" id="677f" class="graf graf--h4 graf--leading"><strong class="markup--strong markup--h4-strong">Create ETL Python¬†Script</strong></h4><p name="71a7" id="71a7" class="graf graf--p graf-after--h4">The first step is consolidating the entire ETL (Extract, Transform, Load) pipeline into a single Python script.</p><p name="88e4" id="88e4" class="graf graf--p graf-after--p">I do that here by defining three files: <em class="markup--em markup--p-em">functions.py, data_pipeline.py</em>, and<em class="markup--em markup--p-em"> requirements.txt</em>. In <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">functions.py</em></strong>, each step of the pipeline is defined as a function. These are then called in sequential order in <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">data_pipeline.py</em></strong>. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">requirements.txt</em></strong> lists all the Python dependencies for the pipeline.</p><figure name="6b7b" id="6b7b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*wgBRc7fKxf2dB6_fpsDd-Q.png" data-width="380" data-height="146" src="https://cdn-images-1.medium.com/max/800/1*wgBRc7fKxf2dB6_fpsDd-Q.png"><figcaption class="imageCaption">Directory tree. Image by¬†author.</figcaption></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="656f" id="656f" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># data_pipeline.py</span><br /><br /><span class="hljs-keyword">from</span> functions <span class="hljs-keyword">import</span> *<br /><span class="hljs-keyword">import</span> time<br /><span class="hljs-keyword">import</span> datetime<br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Starting data pipeline at &quot;</span>, datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>))<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----------------------------------------------&quot;</span>)<br /><br /><span class="hljs-comment"># Step 1: extract video IDs</span><br />t0 = time.time()<br />getVideoIDs()<br />t1 = time.time()<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 1: Done&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&gt; Video IDs downloaded in&quot;</span>, <span class="hljs-built_in">str</span>(t1-t0), <span class="hljs-string">&quot;seconds&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>)<br /><br /><span class="hljs-comment"># Step 2: extract transcripts for videos</span><br />t0 = time.time()<br />getVideoTranscripts()<br />t1 = time.time()<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 2: Done&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&gt; Transcripts downloaded in&quot;</span>, <span class="hljs-built_in">str</span>(t1-t0), <span class="hljs-string">&quot;seconds&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>)<br /><br /><span class="hljs-comment"># Step 3: Transform data</span><br />t0 = time.time()<br />transformData()<br />t1 = time.time()<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 3: Done&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&gt; Data transformed in&quot;</span>, <span class="hljs-built_in">str</span>(t1-t0), <span class="hljs-string">&quot;seconds&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>)<br /><br /><span class="hljs-comment"># Step 4: Generate text emebeddings</span><br />t0 = time.time()<br />createTextEmbeddings()<br />t1 = time.time()<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Step 4: Done&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&gt; Embeddings generated in&quot;</span>, <span class="hljs-built_in">str</span>(t1-t0), <span class="hljs-string">&quot;seconds&quot;</span>, <span class="hljs-string">&quot;\n&quot;</span>)</span></pre><p name="e591" id="e591" class="graf graf--p graf-after--pre">I added time printouts for each step in the pipeline to help with code observability and debugging. I won‚Äôt get into the guts of each step here since those were described in the <a href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" data-href="https://towardsdatascience.com/how-to-build-data-pipelines-for-machine-learning-b97bbef050a5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous article</a> and <a href="https://youtu.be/6qCrvlHRhcM?si=MYHlNOuO-Q19zsuf&amp;t=2054" data-href="https://youtu.be/6qCrvlHRhcM?si=MYHlNOuO-Q19zsuf&amp;t=2054" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this video</a>, but those interested can review the code on <a href="https://github.com/ShawhinT/data-pipeline-example" data-href="https://github.com/ShawhinT/data-pipeline-example" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="6414" id="6414" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Create GitHub¬†Repo</strong></h4><p name="4de6" id="4de6" class="graf graf--p graf-after--h4">Next, we create a GitHub repo. You can do this from the command line or the web interface. I‚Äôll use the latter.</p><p name="a14e" id="a14e" class="graf graf--p graf-after--p">To do this, I go to my GitHub repositories, hit ‚ÄúNew‚Äù, and fill out the following fields.</p><figure name="6ffc" id="6ffc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*W3CXtIutP_SJCXOOnHKfwQ.png" data-width="1224" data-height="903" src="https://cdn-images-1.medium.com/max/800/1*W3CXtIutP_SJCXOOnHKfwQ.png"><figcaption class="imageCaption">Creating GitHub repository. Image by¬†author.</figcaption></figure><p name="2e3f" id="2e3f" class="graf graf--p graf-after--figure">Then, we can clone the repo locally via the following command.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="bash" name="d677" id="d677" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">&gt;&gt; git <span class="hljs-built_in">clone</span> https://github.com/ShawhinT/data-pipeline-example.git</span></pre><h4 name="e293" id="e293" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Create Workflow¬†.yml¬†File</strong></h4><p name="6515" id="6515" class="graf graf--p graf-after--h4">We can now create our workflow via a¬†.yml file. To do that, we will create a new folder called¬†.github/workflows and a new file called <em class="markup--em markup--p-em">data-pipeline.yml</em>.</p><figure name="ed0b" id="ed0b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EEvKm2Z7GGedCPU8NneLTw.png" data-width="402" data-height="246" src="https://cdn-images-1.medium.com/max/800/1*EEvKm2Z7GGedCPU8NneLTw.png"><figcaption class="imageCaption">New directory tree. Image by¬†author.</figcaption></figure><p name="3929" id="3929" class="graf graf--p graf-after--figure">The¬†.yml file looks like this.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="yaml" name="9fe8" id="9fe8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-attr">name:</span> <span class="hljs-string">data-pipeline-workflow</span><br /><br /><span class="hljs-attr">on:</span><br />  <span class="hljs-attr">push:</span> <span class="hljs-comment"># run on push</span><br />  <span class="hljs-attr">schedule:</span><br />    <span class="hljs-bullet">-</span> <span class="hljs-attr">cron:</span> <span class="hljs-string">&quot;35 0 * * *&quot;</span> <span class="hljs-comment"># run every day at 12:35AM</span><br />  <span class="hljs-attr">workflow_dispatch:</span>  <span class="hljs-comment"># manual triggers</span><br /><br /><span class="hljs-attr">jobs:</span><br />  <span class="hljs-attr">run-data-pipeline:</span><br />    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span><br />    <span class="hljs-attr">steps:</span><br />      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Checkout</span> <span class="hljs-string">repo</span> <span class="hljs-string">content</span><br />        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v4</span><br />        <span class="hljs-attr">with:</span><br />          <span class="hljs-attr">token:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.PERSONAL_ACCESS_TOKEN</span> <span class="hljs-string">}}</span>  <span class="hljs-comment"># Use the PAT instead of the default GITHUB_TOKEN</span><br />      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Setup</span> <span class="hljs-string">python</span><br />        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-python@v5</span><br />        <span class="hljs-attr">with:</span><br />          <span class="hljs-attr">python-version:</span> <span class="hljs-string">&#x27;3.9&#x27;</span><br />          <span class="hljs-attr">cache:</span> <span class="hljs-string">&#x27;pip&#x27;</span><br />      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">dependencies</span><br />        <span class="hljs-attr">run:</span> <span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">-r</span> <span class="hljs-string">requirements.txt</span><br />      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">data</span> <span class="hljs-string">pipeline</span><br />        <span class="hljs-attr">env:</span><br />          <span class="hljs-attr">YT_API_KEY:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.YT_API_KEY</span> <span class="hljs-string">}}</span> <span class="hljs-comment"># import API key</span><br />        <span class="hljs-attr">run:</span> <span class="hljs-string">python</span> <span class="hljs-string">data_pipeline.py</span> <span class="hljs-comment"># run data pipeline</span><br />      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Check</span> <span class="hljs-string">for</span> <span class="hljs-string">changes</span> <span class="hljs-comment"># create env variable indicating if any changes were made</span><br />        <span class="hljs-attr">id:</span> <span class="hljs-string">git-check</span><br />        <span class="hljs-attr">run:</span> <span class="hljs-string">|<br />          git config user.name &#x27;github-actions&#x27;<br />          git config user.email &#x27;github-actions@github.com&#x27;<br />          git add .<br />          git diff --staged --quiet || echo &quot;changes=true&quot; &gt;&gt; $GITHUB_ENV <br /></span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Commit</span> <span class="hljs-string">and</span> <span class="hljs-string">push</span> <span class="hljs-string">if</span> <span class="hljs-string">changes</span><br />        <span class="hljs-attr">if:</span> <span class="hljs-string">env.changes</span> <span class="hljs-string">==</span> <span class="hljs-string">&#x27;true&#x27;</span> <span class="hljs-comment"># if changes made push new data to repo</span><br />        <span class="hljs-attr">run:</span> <span class="hljs-string">|<br />          git commit -m &quot;updated video index&quot;<br />          git push</span></span></pre><p name="9a2a" id="9a2a" class="graf graf--p graf-after--pre">While this may seem overwhelming to those new to GitHub Actions, it consists of <strong class="markup--strong markup--p-strong">4 key elements: name, triggers, jobs, and steps</strong>.</p><p name="752c" id="752c" class="graf graf--p graf-after--p">Starting with the <strong class="markup--strong markup--p-strong">name</strong>, this is the name of our workflow. Here, I call it <em class="markup--em markup--p-em">‚Äúdata-pipeline-workflow‚Äù</em>.</p><p name="87e7" id="87e7" class="graf graf--p graf-after--p">Next, we define <strong class="markup--strong markup--p-strong">triggers</strong> for the workflow using the ‚Äú<em class="markup--em markup--p-em">on:</em>‚Äù syntax. Here, I define three separate triggers. <strong class="markup--strong markup--p-strong">1) on push</strong>, meaning the workflow will execute when new code is pushed to the repo. <strong class="markup--strong markup--p-strong">2) on schedule</strong>, meaning it will execute via a cron job. <strong class="markup--strong markup--p-strong">3) on workflow_dispatch</strong>, this will create a button in our repo we can manually click to run the workflow.</p><p name="4ff8" id="4ff8" class="graf graf--p graf-after--p">Workflows are made up of <strong class="markup--strong markup--p-strong">jobs</strong>. In this example, we have a single job called ‚Äú<em class="markup--em markup--p-em">run-data-pipeline</em>‚Äù. We can set the operating system for the job (I use Ubuntu here).</p><p name="e124" id="e124" class="graf graf--p graf-after--p">Jobs are then made up of <strong class="markup--strong markup--p-strong">steps</strong>. Here, our job consists of 6 steps, which are listed below.</p><ol class="postList"><li name="ee55" id="ee55" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Checkout repo content</strong>: uses the <a href="https://github.com/actions/checkout" data-href="https://github.com/actions/checkout" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">pre-built checkout action</a> to pull code from the repo. I also provide a Personal Access Token (PAT) to allow the action to push code to the repo, which we will create momentarily.</li><li name="0cdb" id="0cdb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Setup Python</strong>: install the desired Python version with the <a href="https://github.com/actions/setup-python" data-href="https://github.com/actions/setup-python" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">pre-built action</a>.</li><li name="5db6" id="5db6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Install dependencies</strong>: install Python libraries listed in <em class="markup--em markup--li-em">requirements.txt</em>.</li><li name="8834" id="8834" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Run data pipeline</strong>: executes the <em class="markup--em markup--li-em">data_pipeline.py</em> script. I add a secret environment variable to allow the action to use my YouTube API token without exposing it on a public repo. I will show you how to do this next.</li><li name="1920" id="1920" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Check for changes</strong>: checks if any changes were made to the repo. This is necessary because if we try to push code to the repo when there are no differences, it will throw an error, and the job will fail to run.</li><li name="2aa4" id="2aa4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Commit and push if changes</strong>: if there are changes to the repo, they will be committed and pushed.</li></ol><p name="4df5" id="4df5" class="graf graf--p graf-after--li">One last thing I‚Äôll do is create a folder called ‚Äúdata‚Äù where we can save the final data files.</p><figure name="371d" id="371d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rHO7HhcvIIT7MlNgGvg1zA.png" data-width="399" data-height="289" src="https://cdn-images-1.medium.com/max/800/1*rHO7HhcvIIT7MlNgGvg1zA.png"><figcaption class="imageCaption">Updated directory tree with data folder. Image by¬†author.</figcaption></figure><h4 name="14f6" id="14f6" class="graf graf--h4 graf-after--figure">Add Repo¬†Secrets</h4><p name="a1f5" id="a1f5" class="graf graf--p graf-after--h4">Notice in the data-pipeline.yml file I referenced two strange-looking variables, e.g., <em class="markup--em markup--p-em">${{ secrets.PERSONAL_ACCESS_TOKEN }}</em> and <em class="markup--em markup--p-em">${{ secrets.YT_API_KEY }}.</em></p><p name="5784" id="5784" class="graf graf--p graf-after--p">These are <strong class="markup--strong markup--p-strong">repository secrets</strong> that are accessible to GitHub Actions as environment variables. To create them, we go to our repository settings, click Secrets and Variables, select Actions, and click ‚ÄúNew repository secrets‚Äù.</p><figure name="3966" id="3966" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mGBdM2iuPSWyuLR8H73Qkw.png" data-width="1691" data-height="804" src="https://cdn-images-1.medium.com/max/800/1*mGBdM2iuPSWyuLR8H73Qkw.png"><figcaption class="imageCaption">Creating a new repository secret. Image by¬†author.</figcaption></figure><p name="0883" id="0883" class="graf graf--p graf-after--figure">To create the <em class="markup--em markup--p-em">PERSONAL_ACCESS_TOKEN</em> variable, click on your profile icon in the top right-hand corner &gt; open Settings in a new tab &gt; scroll to the bottom, and select ‚ÄúDeveloper settings‚Äù &gt; click ‚ÄúPersonal access tokens‚Äù &gt; click ‚ÄúGenerate new token‚Äù (classic). This will allow you to create a token that gives the Actions write access to your repo.</p><p name="75de" id="75de" class="graf graf--p graf-after--p">Here are the details I used.</p><ul class="postList"><li name="ef56" id="ef56" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Name</strong>: data-pipeline-example-PAT</li><li name="dde3" id="dde3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Expiration</strong>: No expiration (feel free to set this to end at some point)</li><li name="9760" id="9760" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Select scopes</strong>: repo</li></ul><p name="5a72" id="5a72" class="graf graf--p graf-after--li">Then click ‚ÄúGenerate token‚Äù at the bottom of the page. This will display a long string of text, which you can copy and paste into your GitHub repository secret.</p><figure name="612f" id="612f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OeYhQSEDkgRX0bmjL-eexA.png" data-width="756" data-height="729" src="https://cdn-images-1.medium.com/max/800/1*OeYhQSEDkgRX0bmjL-eexA.png"><figcaption class="imageCaption">Creating <em class="markup--em markup--figure-em">PERSONAL_ACCESS_TOKEN. Image by¬†author.</em></figcaption></figure><p name="267d" id="267d" class="graf graf--p graf-after--figure">I then create another secret variable called <em class="markup--em markup--p-em">YT_API_KEY </em>similarly<em class="markup--em markup--p-em">.</em></p><h4 name="17dc" id="17dc" class="graf graf--h4 graf-after--p">Push and¬†commit</h4><p name="8f34" id="8f34" class="graf graf--p graf-after--h4">With our secrets in place, we can commit and push our code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="bash" name="9b84" id="9b84" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">&gt;&gt; git add .<br />&gt;&gt; git commit -m <span class="hljs-string">&quot;adding data pipeline code&quot;</span><br />&gt;&gt; git push</span></pre><p name="0341" id="0341" class="graf graf--p graf-after--pre">Once pushed, we can go to the repo&#39;s ‚ÄúActions‚Äù tab and watch the workflow run!</p><div name="535d" id="535d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/data-pipeline-example" data-href="https://github.com/ShawhinT/data-pipeline-example" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/data-pipeline-example"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - ShawhinT/data-pipeline-example: Example data pipeline automation with GitHub Actions</strong><br><em class="markup--em markup--mixtapeEmbed-em">Example data pipeline automation with GitHub Actions - ShawhinT/data-pipeline-example</em>github.com</a><a href="https://github.com/ShawhinT/data-pipeline-example" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="819bc973786781907f670355a374197a" data-thumbnail-img-id="0*T_nslNL53TMNSY52" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*T_nslNL53TMNSY52);"></a></div><h3 name="ce69" id="ce69" class="graf graf--h3 graf-after--mixtapeEmbed">What‚Äôs next?</h3><p name="61f9" id="61f9" class="graf graf--p graf-after--h3">While orchestration tools are commonplace in data engineering, these might be overkill for some ML use cases. Here, we saw a free and simple way to automate a data pipeline using Python and GitHub Actions.</p><p name="ec74" id="ec74" class="graf graf--p graf-after--p">In the <a href="https://towardsdatascience.com/how-to-deploy-ml-solutions-with-fastapi-docker-and-gcp-de1bb8bfc59a" data-href="https://towardsdatascience.com/how-to-deploy-ml-solutions-with-fastapi-docker-and-gcp-de1bb8bfc59a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">next article</a> of this series, we will continue going down the data science tech stack and discuss how we can integrate this data pipeline into a semantic search system for my YouTube videos.</p><p name="1d22" id="1d22" class="graf graf--p graf-after--p">More in this series üëá</p><div name="b3f9" id="b3f9" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/f0910c75d006" data-href="https://shawhin.medium.com/list/f0910c75d006" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/f0910c75d006"><strong class="markup--strong markup--mixtapeEmbed-strong">Full Stack Data Science</strong><br><em class="markup--em markup--mixtapeEmbed-em">Edit description</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/f0910c75d006" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="967361bcbe2dbfbd874fe579e7de5500" data-thumbnail-img-id="0*0c04fcb9f5df93a23415d77f585da8eec4dca340.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*0c04fcb9f5df93a23415d77f585da8eec4dca340.jpeg);"></a></div></div></div></section><section name="e561" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1967" id="1967" class="graf graf--h3 graf--leading">Resources</h3><p name="71be" id="71be" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Connect</strong>: <a href="https://shawhintalebi.com/" data-href="https://shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My website</a> | <a href="https://calendly.com/shawhintalebi" data-href="https://calendly.com/shawhintalebi" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Book a call</a></p><p name="5a52" id="5a52" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Socials</strong>: <a href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" data-href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">YouTube üé•</a> | <a href="https://www.linkedin.com/in/shawhintalebi/" data-href="https://www.linkedin.com/in/shawhintalebi/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://twitter.com/ShawhinT" data-href="https://twitter.com/ShawhinT" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Twitter</a></p><p name="6a04" id="6a04" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Support</strong>: <a href="https://www.buymeacoffee.com/shawhint" data-href="https://www.buymeacoffee.com/shawhint" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Buy me a coffee</a> ‚òïÔ∏è</p><div name="fe26" id="fe26" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/subscribe"><strong class="markup--strong markup--mixtapeEmbed-strong">Get FREE access to every new story I write</strong><br><em class="markup--em markup--mixtapeEmbed-em">Get FREE access to every new story I write P.S. I do not share your email with anyone By signing up, you will create a‚Ä¶</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/subscribe" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d6d6c2d301f2c367b618a850fa468b3" data-thumbnail-img-id="0*O69FxQ5FttNEA_r4" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*O69FxQ5FttNEA_r4);"></a></div></div></div></section><section name="4622" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="469a" id="469a" class="graf graf--p graf--leading graf--trailing">[1] <a href="https://youtu.be/oBklltKXtDE?si=a7aQZ903orc0nqmM" data-href="https://youtu.be/oBklltKXtDE?si=a7aQZ903orc0nqmM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch at Tesla‚Ää‚Äî‚ÄäAndrej Karpathy</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/c19e2ef9ca90"><time class="dt-published" datetime="2024-05-30T18:51:56.365Z">May 30, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/automating-data-pipelines-with-python-github-actions-c19e2ef9ca90" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on March 8, 2025.</p></footer></article></body></html>