[
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "This is the 2nd article in a larger series on multimodal AI. In the previous post, we saw how to augment large language models (LLMs) to understand new data modalities (e.g., images, audio, video). One such approach relied on encoders that generate vector representations (i.e. embeddings) of non-text data. In this article, I will discuss multimodal embeddings and share what they can do via two practical use cases."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "AI research is traditionally split into distinct fields: NLP, computer vision (CV), robotics, human-computer interface (HCI), etc. However, countless practical tasks require the integration of these different research areas e.g. autonomous vehicles (CV + robotics), AI agents (NLP + CV + HCI), personalized learning (NLP + HCI), etc."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "Although these fields aim to solve different problems and work with different data types, they all share a fundamental process. Namely, generating useful numerical representations of real-world phenomena."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "Historically, this was done by hand. This means that researchers and practitioners would use their (or other people‚Äôs) expertise to explicitly transform data into a more helpful form. Today, however, these can be derived another way."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "Embeddings are (useful) numerical representations of data learned implicitly through model training. For example, through learning how to predict text, BERT learned representations of text, which are helpful for many NLP tasks [1]. Another example is the Vision Transformer (ViT), trained for image classification on Image Net, which can be repurposed for other applications [2]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "A key point here is that these learned embedding spaces will have some underlying structure so that similar concepts are located close together. As shown in the toy examples below."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "One key limitation of the previously mentioned models is they are restricted to a single data modality, e.g., text or images. Preventing cross-modal applications like image captioning, content moderation, image search, and more. But what if we could merge these two representations?"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "Although text and images may look very different to us, in a neural network, these are represented via the same mathematical object, i.e., a vector. Therefore, in principle, text, images, or any other data modality can processed by a single model."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "This fact underlies multimodal embeddings, which represent multiple data modalities in the same vector space such that similar concepts are co-located (independent of their original representations)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "For example, CLIP encodes text and images into a shared embedding space [3]. A key insight from CLIP is that by aligning text and image representations, the model is capable of 0-shot image classification on an arbitrary set of target classes since any input text can be treated as a class label (we will see a concrete example of this later)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "However, this idea is not limited to text and images. Virtually any data modalities can be aligned in this way e.g., text-audio, audio-image, text-EEG, image-tabular, and text-video. Unlocking use cases such as video captioning, advanced OCR, audio transcription, video search, and EEG-to-text [4]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "The standard approach to aligning disparate embedding spaces is contrastive learning (CL). A key intuition of CL is to represent different views of the same information similarly [5]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "This consists of learning representations that maximize the similarity between positive pairs and minimize the similarity of negative pairs. In the case of an image-text model, a positive pair might be an image with an appropriate caption, while a negative pair would be an image with an irrelevant caption (as shown below)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "Two key aspects of CL contribute to its effectiveness"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "Since positive and negative pairs can be curated from the data‚Äôs inherent structure (e.g., metadata from web images), CL training data do not require manual labeling, which unlocks larger-scale training and more powerful representations [3].It simultaneously maximizes positive and minimizes negative pair similarity via a special loss function, as demonstrated by CLIP [3]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "With a high-level understanding of how multimodal embeddings work, let‚Äôs see two concrete examples of what they can do. Here, I will use the open-source CLIP model to perform two tasks: 0-shot image classification and image search."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "The code for these examples is freely available on the GitHub repository."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "The basic idea behind using CLIP for 0-shot image classification is to pass an image into the model along with a set of possible class labels. Then, a classification can be made by evaluating which text input is most similar to the input image."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "We‚Äôll start by importing the Hugging Face Transformers library so that the CLIP model can be downloaded locally. Additionally, the PIL library is used to load images in Python."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "Next, we can import a version of the clip model and its associated data processor. Note: the processor handles tokenizing input text and image preparation."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "We load in the below image of a cat and create a list of two possible class labels: ‚Äúa photo of a cat‚Äù or ‚Äúa photo of a dog‚Äù."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "Next, we‚Äôll preprocess the image/text inputs and pass them into the model."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "To make a class prediction, we must extract the image logits and evaluate which class corresponds to the maximum."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "The model nailed it with a 99.79% probability that it‚Äôs a cat photo. However, this was a super easy one. Let‚Äôs see what happens when we change the class labels to: ‚Äúugly cat‚Äù and ‚Äúcute cat‚Äù for the same image."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "The model easily identified that the image was indeed a cute cat. Let‚Äôs do something more challenging like the labels: ‚Äúcat meme‚Äù or ‚Äúnot cat meme‚Äù."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "While the model is less confident about this prediction with a 54.64% probability, it correctly implies that the image is not a meme."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "Another application of CLIP is essentially the inverse of Use Case 1. Rather than identifying which text label matches an input image, we can evaluate which image (in a set) best matches a text input (i.e. query)‚Äîin other words, performing a search over images."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "We start by storing a set of images in a list. Here, I have three images of a cat, dog, and goat, respectively."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "Next, we can define a query like ‚Äúa cute dog‚Äù and pass it and the images into CLIP."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "We can then match the best image to the input text by extracting the text logits and evaluating the image corresponding to the maximum."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "We see that (again) the model nailed this simple example. But let‚Äôs try some trickier examples."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image¬†search",
        "text": "Although this last prediction is quite controversial, all the other matches were spot on! This is likely since images like these are ubiquitous on the internet and thus were seen many times in CLIP‚Äôs pre-training."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "Multimodal embeddings unlock countless AI use cases that involve multiple data modalities. Here, we saw two such use cases, i.e., 0-shot image classification and image search using CLIP."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "Another practical application of models like CLIP is multimodal RAG, which consists of the automated retrieval of multimodal context to an LLM. In the next article of this series, we will see how this works under the hood and review a concrete example."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "My website: https://www.shawhintalebi.com/"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "[1] BERT[2] ViT[3] CLIP[4] Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)[5] A Simple Framework for Contrastive Learning of Visual Representations"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "By Shaw Talebi on November 29, 2024."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What‚Äôs Next?",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Multimodal Models‚Ää‚Äî‚ÄäLLMs That Can See and¬†Hear",
        "text": "This is the first post in a larger series on Multimodal AI. A Multimodal Model (MM) is an AI system capable of processing or generating multiple data modalities (e.g., text, image, audio, video). In this article, I will discuss a particular type of MM that builds on top of a large language model (LLM). I‚Äôll start with a high-level overview of such models and then share example code for using LLaMA 3.2 Vision to perform various image-to-text tasks."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Multimodal Models‚Ää‚Äî‚ÄäLLMs That Can See and¬†Hear",
        "text": "Large language models (LLMs) have marked a fundamental shift in AI research and development. However, despite their broader impacts, they are still fundamentally limited."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Multimodal Models‚Ää‚Äî‚ÄäLLMs That Can See and¬†Hear",
        "text": "Namely, LLMs can only process and generate text, making them blind to other modalities such as images, video, audio, and more. This is a major limitation since some tasks rely on non-text data, e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Multimodal Models‚Ää‚Äî‚ÄäLLMs That Can See and¬†Hear",
        "text": "This has sparked efforts toward expanding LLM functionality to include multiple modalities."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "GPT-4o‚Ää‚Äî‚ÄäInput: text, images, and audio. Output: text.FLUX‚Ää‚Äî‚ÄäInput: text. Output: images.Suno‚Ää‚Äî‚ÄäInput: text. Output: audio."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "One benefit of using existing LLM as a starting point for MMs is that they‚Äôve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "3 Paths to Multimodality",
        "text": "Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "3 Paths to Multimodality",
        "text": "LLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-training"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 1: LLM +¬†Tools",
        "text": "The simplest way to make an LLM multimodal is by adding external modules that can readily translate between text and an arbitrary modality. For example, a transcription model (e.g. Whisper) can be connected to an LLM to translate input speech into text, or a text-to-image model can generate images based on LLM outputs."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 1: LLM +¬†Tools",
        "text": "The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 1: LLM +¬†Tools",
        "text": "The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 2: LLM +¬†Adapters",
        "text": "One way to mitigate the ‚Äútelephone problem‚Äù is by optimizing the representations of new modalities to align with the LLM‚Äôs internal concept space. For example, ensuring an image of a dog and the description of one look similar to the LLM."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 2: LLM +¬†Adapters",
        "text": "This is possible through the use of adapters, a relatively small set of parameters that appropriately translate a dense vector representation for a downstream model [2][4][5]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 2: LLM +¬†Adapters",
        "text": "Adapters can be trained using, for example, image-caption pairs, where the adapter learns to translate an image encoding into a representation compatible with the LLM [2][4][6]. One way to achieve this is via contrastive learning [2], which I will discuss more in the next article of this series."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 2: LLM +¬†Adapters",
        "text": "The benefits of using adapters to augment LLMs include better alignment between novel modality representations in a data-efficient way. Since many pre-trained embedding, language, and diffusion models are available in today‚Äôs AI landscape, one can readily fuse models based on their needs. Notable examples from the open-source community are LLaVA, LLaMA 3.2 Vision, Flamingo, MiniGPT4, Janus, Mini-Omni2, and IDEFICS [3][5][7][8]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 2: LLM +¬†Adapters",
        "text": "However, this data efficiency comes at a price. Just like how adapter-based fine-tuning approaches (e.g. LoRA) can only nudge an LLM so far, the same holds in this context. Additionally, pasting various encoders and decoders to an LLM may result in overly complicated model architectures."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 3: Unified¬†Models",
        "text": "The final way to make an LLM multimodal is by incorporating multiple modalities at the pre-training stage. This works by adding modality-specific tokenizers (rather than pre-trained encoder/decoder models) to the model architecture and expanding the embedding layer to accommodate new modalities [9]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 3: Unified¬†Models",
        "text": "While this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 3: Unified¬†Models",
        "text": "The preeminent example of this unified approach is (presumably) GPT-4o, which processes text, image, and audio inputs to enable expanded reasoning capabilities at faster inference times than previous versions of GPT-4. Other models that follow this approach include Gemini, Emu3, BLIP, and Chameleon [9][10]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Path 3: Unified¬†Models",
        "text": "Training these models typically entails multi-step pre-training on a set of (multimodal) tasks, such as language modeling, text-image contrastive learning, text-to-video generation, and others [7][9][10]."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "With a basic understanding of how LLM-based multimodal models work under the hood, let‚Äôs see what we can do with them. Here, I will use LLaMA 3.2 Vision to perform various image-to-text tasks."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "To run this example, download Ollama and its Python library. This enables the model to run locally i.e. no need for external API calls."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The example code is freely available on GitHub."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Next, we‚Äôll download the model locally. Here, we use LLaMA 3.2 Vision 11B."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Now, we‚Äôre ready to use the model! Here‚Äôs how we can do basic visual question answering."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The image is of me from a networking event (as shown below)."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The model‚Äôs response is shown below. While it has trouble reading what‚Äôs on my hat, it does a decent job inferring the context of the photo."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "If you run this on your machine, you may run into a long wait time until the model generates a response. One thing we can do to make this less painful is to enable streaming."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Interestingly, we get a qualitatively different response when prompting the model in a slightly different way for the same image."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Objectively describing a scene is simpler than understanding and explaining humor. Let‚Äôs see how the model explains the meme below."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The model does a good job here. It understands that the image is funny while also conveying the pain that people face."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The last use case is optical character recognition (OCR). This involves extracting text from images, which is valuable in a wide range of contexts. Here, I‚Äôll see if the model can translate a screenshot from my notes app to a markdown file."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Again, the model does a decent job out of the box. While it missed the header, it accurately captured the content and formatting of the project ideas."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs)."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "However, there are other types of multimodal models. In the next article of this series, I will discuss multimodal embedding models, which encode multiple data modalities (e.g. text and images) into a shared representation space."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "üëâ Get FREE access to every new story I write (Learn More)"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[1] Multimodal Machine Learning: A Survey and Taxonomy"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[2] A Survey on Multimodal Large Language Models"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[5] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[6] Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[7] Flamingo: a Visual Language Model for Few-Shot Learning"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[8] Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[9] Emu3: Next-Token Prediction is All You Need"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "[10] Chameleon: Mixed-Modal Early-Fusion Foundation Models"
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "By Shaw Talebi on November 19, 2024."
    },
    {
        "article_title": "Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear",
        "section": "What‚Äôs next?",
        "text": "Exported from Medium on December 2, 2024."
    }
]