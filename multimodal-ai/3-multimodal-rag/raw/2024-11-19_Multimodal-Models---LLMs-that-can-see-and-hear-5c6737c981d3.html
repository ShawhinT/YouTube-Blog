<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear</h1>
</header>
<section data-field="subtitle" class="p-summary">
An introduction with example Python code
</section>
<section data-field="body" class="e-content">
<section name="d85b" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2ec2" id="2ec2" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Multimodal Models</strong>‚Ää‚Äî‚Ää<strong class="markup--strong markup--h3-strong">LLMs That Can See and¬†Hear</strong></h3><h4 name="cbc8" id="cbc8" class="graf graf--h4 graf-after--h3 graf--subtitle">An introduction with example Python¬†code</h4><p name="afb0" id="afb0" class="graf graf--p graf-after--h4">This is the first post in a larger series on <a href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" data-href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Multimodal AI</a>. A <strong class="markup--strong markup--p-strong">Multimodal Model (MM)</strong> is an <strong class="markup--strong markup--p-strong">AI system capable of processing or generating multiple data modalities</strong> (e.g., text, image, audio, video). In this article, I will discuss a particular type of MM that builds on top of a large language model (LLM). I‚Äôll start with a high-level overview of such models and then share example code for using LLaMA 3.2 Vision to perform various image-to-text tasks.</p><figure name="1e63" id="1e63" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="0*YE-Q-OuWnrgrUrQw" data-width="4193" data-height="3145" data-unsplash-photo-id="JZSBzvFW-2Q" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*YE-Q-OuWnrgrUrQw"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@sincerelymedia?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@sincerelymedia?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Sincerely Media</a> on¬†<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section><section name="71e4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="249f" id="249f" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/Ot2c5MKN_-w?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="7cd0" id="7cd0" class="graf graf--p graf-after--figure"><a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" data-href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Large language models (LLMs)</a> have marked a fundamental shift in AI research and development. However, despite their broader impacts, they are still <strong class="markup--strong markup--p-strong">fundamentally limited</strong>.</p><p name="6faf" id="6faf" class="graf graf--p graf-after--p">Namely, LLMs can only process and generate text, making them <em class="markup--em markup--p-em">blind</em> to other modalities such as images, video, audio, and more. This is a major limitation since <strong class="markup--strong markup--p-strong">some tasks rely on non-text data,</strong> e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics.</p><p name="4606" id="4606" class="graf graf--p graf-after--p">This has sparked efforts toward expanding LLM functionality to include multiple modalities.</p><h3 name="1ed2" id="1ed2" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">What is a Multimodal Model?</strong></h3><p name="abff" id="abff" class="graf graf--p graf-after--h3">A <strong class="markup--strong markup--p-strong">Multimodal Model (MM)</strong> is an <strong class="markup--strong markup--p-strong">AI system that can process multiple data modalities as input or output (or both)</strong> [1]. Below are a few examples.</p><ul class="postList"><li name="2b4f" id="2b4f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">GPT-4o</strong>‚Ää‚Äî‚ÄäInput: text, images, and audio. Output: text.</li><li name="2509" id="2509" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">FLUX</strong>‚Ää‚Äî‚ÄäInput: text. Output: images.</li><li name="080f" id="080f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Suno</strong>‚Ää‚Äî‚ÄäInput: text. Output: audio.</li></ul><figure name="ef79" id="ef79" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*yvfu8VAp1UgCw4SVvUe77Q.png" data-width="3478" data-height="1140" src="https://cdn-images-1.medium.com/max/800/1*yvfu8VAp1UgCw4SVvUe77Q.png"><figcaption class="imageCaption">Example mutlimodal models. Image by¬†author.</figcaption></figure><p name="3026" id="3026" class="graf graf--p graf-after--figure">While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use <strong class="markup--strong markup--p-strong">LLMs as the core reasoning engine of a multimodal system</strong> [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].</p><p name="9658" id="9658" class="graf graf--p graf-after--p">One benefit of using existing LLM as a starting point for MMs is that they‚Äôve <strong class="markup--strong markup--p-strong">demonstrated a strong ability to acquire world knowledge through large-scale pre-training</strong>, which can be leveraged to process concepts appearing in non-textual representations.</p><h3 name="f49d" id="f49d" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">3 Paths to Multimodality</strong></h3><p name="e001" id="e001" class="graf graf--p graf-after--h3">Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below.</p><ol class="postList"><li name="5d9f" id="5d9f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">LLM + Tools</strong>: Augment LLMs with pre-built components</li><li name="0a67" id="0a67" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">LLM + Adapters</strong>: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuning</li><li name="86b8" id="86b8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Unified Models</strong>: Expand LLM architecture to fuse modalities at pre-training</li></ol><h3 name="b346" id="b346" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">Path 1: LLM +¬†Tools</strong></h3><p name="5ed2" id="5ed2" class="graf graf--p graf-after--h3">The simplest way to make an LLM multimodal is by <strong class="markup--strong markup--p-strong">adding external modules that can readily translate between text and an arbitrary modality</strong>. For example, a transcription model (e.g. Whisper) can be connected to an LLM to translate input speech into text, or a text-to-image model can generate images based on LLM outputs.</p><p name="d24b" id="d24b" class="graf graf--p graf-after--p">The key benefit of such an approach is <strong class="markup--strong markup--p-strong">simplicity</strong>. Tools can quickly be assembled without any additional model training.</p><p name="f66e" id="f66e" class="graf graf--p graf-after--p">The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. <strong class="markup--strong markup--p-strong">Information may degrade going from one module to another via text descriptions only.</strong></p><figure name="c4fb" id="c4fb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Nwc-ZhRFKH17LWWmsNhbdA.png" data-width="3312" data-height="1332" src="https://cdn-images-1.medium.com/max/800/1*Nwc-ZhRFKH17LWWmsNhbdA.png"><figcaption class="imageCaption">An example of information degradation during message passing. Image by¬†author.</figcaption></figure><h3 name="f683" id="f683" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Path 2: LLM +¬†Adapters</strong></h3><p name="010d" id="010d" class="graf graf--p graf-after--h3">One way to mitigate the ‚Äútelephone problem‚Äù is by optimizing the representations of new modalities to align with the LLM‚Äôs internal concept space. For example, ensuring an image of a dog and the description of one <em class="markup--em markup--p-em">look</em> similar to the LLM.</p><p name="8415" id="8415" class="graf graf--p graf-after--p">This is possible through the use of <strong class="markup--strong markup--p-strong">adapters</strong>, a relatively small set of <strong class="markup--strong markup--p-strong">parameters that appropriately translate a dense vector representation for a downstream model</strong> [2][4][5].</p><p name="3edc" id="3edc" class="graf graf--p graf-after--p">Adapters can be trained using, for example, image-caption pairs, where the adapter learns to translate an image encoding into a representation compatible with the LLM [2][4][6]. One way to achieve this is via contrastive learning [2], which I will discuss more in the <a href="https://towardsdatascience.com/multimodal-embeddings-an-introduction-5dc36975966f" data-href="https://towardsdatascience.com/multimodal-embeddings-an-introduction-5dc36975966f" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">next article</a> of this series.</p><figure name="ea2a" id="ea2a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pyqGh5Cbrk_EMlPYtrfrQw.png" data-width="2484" data-height="1174" src="https://cdn-images-1.medium.com/max/800/1*pyqGh5Cbrk_EMlPYtrfrQw.png"><figcaption class="imageCaption">A simple strategy for integrating images into an LLM via an image encoding adapter. Image by¬†author.</figcaption></figure><p name="9e6e" id="9e6e" class="graf graf--p graf-after--figure">The benefits of using adapters to augment LLMs include <strong class="markup--strong markup--p-strong">better alignment between novel modality representations in a data-efficient way</strong>. Since many pre-trained embedding, language, and diffusion models are available in today‚Äôs AI landscape, one can readily fuse models based on their needs. Notable examples from the open-source community are LLaVA, LLaMA 3.2 Vision, Flamingo, MiniGPT4, Janus, Mini-Omni2, and IDEFICS [3][5][7][8].</p><p name="314f" id="314f" class="graf graf--p graf-after--p">However, this data efficiency comes at a price. Just like how adapter-based fine-tuning approaches (e.g. <a href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#8e86" data-href="https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91#8e86" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LoRA</a>) can only nudge an LLM so far, the same holds in this context. Additionally, pasting various encoders and decoders to an LLM may result in overly complicated model architectures.</p><h3 name="6097" id="6097" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Path 3: Unified¬†Models</strong></h3><p name="d54e" id="d54e" class="graf graf--p graf-after--h3">The final way to make an LLM multimodal is by <strong class="markup--strong markup--p-strong">incorporating multiple modalities at the pre-training stage</strong>. This works by adding modality-specific tokenizers (rather than pre-trained encoder/decoder models) to the model architecture and expanding the embedding layer to accommodate new modalities [9].</p><p name="8c47" id="8c47" class="graf graf--p graf-after--p">While this approach comes with significantly greater technical challenges and computational requirements, it enables the <strong class="markup--strong markup--p-strong">seamless integration of multiple modalities into a shared concept space</strong>, unlocking better reasoning capabilities and efficiencies [10].</p><p name="18fa" id="18fa" class="graf graf--p graf-after--p">The preeminent example of this unified approach is (presumably) GPT-4o, which processes text, image, and audio inputs to enable <strong class="markup--strong markup--p-strong">expanded reasoning capabilities at faster inference times than previous versions of GPT-4</strong>. Other models that follow this approach include Gemini, Emu3, BLIP, and Chameleon [9][10].</p><p name="4876" id="4876" class="graf graf--p graf-after--p">Training these models typically entails multi-step pre-training on a set of (multimodal) tasks, such as language modeling, text-image contrastive learning, text-to-video generation, and others [7][9][10].</p><h3 name="e550" id="e550" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Example: Using </strong>LLaMA 3.2 Vision for Image-based Tasks</h3><p name="4e2c" id="4e2c" class="graf graf--p graf-after--h3">With a basic understanding of how LLM-based multimodal models work under the hood, let‚Äôs see what we can do with them. Here, I will use LLaMA 3.2 Vision to perform various image-to-text tasks.</p><p name="6c11" id="6c11" class="graf graf--p graf-after--p">To run this example, <a href="https://ollama.com/download" data-href="https://ollama.com/download" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">download Ollama</a> and its <a href="https://pypi.org/project/ollama/" data-href="https://pypi.org/project/ollama/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Python library</a>. This enables the model to run locally i.e. no need for external API calls.</p><p name="6404" id="6404" class="graf graf--p graf-after--p graf--trailing">The example code is freely available on <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section><section name="3594" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="6613" id="6613" class="graf graf--h4 graf--leading">Importing model</h4><p name="e4c9" id="e4c9" class="graf graf--p graf-after--h4">We start by importing ollama.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b370" id="b370" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> ollama</span></pre><p name="0d34" id="0d34" class="graf graf--p graf-after--pre">Next, we‚Äôll download the model locally. Here, we use LLaMA 3.2 Vision 11B.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f324" id="f324" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">ollama.pull(<span class="hljs-string">&#x27;llama3.2-vision&#x27;</span>)</span></pre><h4 name="b5c7" id="b5c7" class="graf graf--h4 graf-after--pre">Visual QA</h4><p name="caf6" id="caf6" class="graf graf--p graf-after--h4">Now, we‚Äôre ready to use the model! Here‚Äôs how we can do basic visual question answering.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9642" id="9642" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># pass image and question to model</span><br />response = ollama.chat(<br />    model=<span class="hljs-string">&#x27;llama3.2-vision&#x27;</span>,<br />    messages=[{<br />        <span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>,<br />        <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;What is in this image?&#x27;</span>,<br />        <span class="hljs-string">&#x27;images&#x27;</span>: [<span class="hljs-string">&#x27;images/shaw-sitting.jpeg&#x27;</span>]<br />    }]<br />)<br /><br /><span class="hljs-comment"># print response</span><br /><span class="hljs-built_in">print</span>(response[<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>])</span></pre><p name="238e" id="238e" class="graf graf--p graf-after--pre">The image is of me from a networking event (as shown below).</p><figure name="83bb" id="83bb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lvX8Mut8SQ1vDhsaewLQ_g.jpeg" data-width="800" data-height="1016" src="https://cdn-images-1.medium.com/max/800/1*lvX8Mut8SQ1vDhsaewLQ_g.jpeg"><figcaption class="imageCaption">Image of me from networking event at Richardson IQ. Image by¬†author.</figcaption></figure><p name="88e1" id="88e1" class="graf graf--p graf-after--figure">The model‚Äôs response is shown below. While it has trouble reading what‚Äôs on my hat, it does a decent job inferring the context of the photo.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="f1f0" id="f1f0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">This image shows a man sitting on a yellow ottoman with his hands clasped <br />together. He is wearing a black polo shirt with a name tag that says &quot;Shaw&quot; <br />and a black baseball cap with white text that reads, &quot;THE DATA ENREPRENEUR.&quot; <br />The background of the image appears to be an office or lounge area, with a <br />large screen on the wall behind him displaying a presentation slide. There are <br />also several chairs and tables in the background, suggesting that this may be <br />a meeting room or common area for employees to gather and work.</span></pre><p name="331e" id="331e" class="graf graf--p graf-after--pre">If you run this on your machine, you may run into a long wait time until the model generates a response. One thing we can do to make this less painful is to enable streaming.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="89f6" id="89f6" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create stream</span><br />stream = ollama.chat(<br />    model=<span class="hljs-string">&#x27;llama3.2-vision&#x27;</span>,<br />    messages=[{<br />        <span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>,<br />        <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Can you write a caption for this image?&#x27;</span>,<br />        <span class="hljs-string">&#x27;images&#x27;</span>: [<span class="hljs-string">&#x27;images/shaw-sitting.jpeg&#x27;</span>]<br />    }],<br />    stream=<span class="hljs-literal">True</span>,<br />)<br /><br /><span class="hljs-comment"># print chunks in stream as they become available</span><br /><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> stream:<br />    <span class="hljs-built_in">print</span>(chunk[<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>], end=<span class="hljs-string">&#x27;&#x27;</span>, flush=<span class="hljs-literal">True</span>)</span></pre><p name="a3b4" id="a3b4" class="graf graf--p graf-after--pre">Interestingly, we get a qualitatively different response when prompting the model in a slightly different way for the same image.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="ca84" id="ca84" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">This image features a man sitting on a yellow chair. He is wearing a black <br />polo shirt with a blue name tag that says &quot;Shaw&quot;, khaki pants, and a black <br />baseball cap with white text that reads &quot;THE DATA ENTHUSIAST&quot;. The man has his <br />hands clasped together in front of him and appears to be smiling.<br /><br />The background of the image consists of a room with various pieces of <br />furniture. There is a green ottoman to the left of the yellow chair, and two <br />blue chairs on the right side of the image. A brown table or desk sits behind <br />the man, along with a fireplace. The walls are painted teal blue and have a <br />wooden accent wall featuring holes for hanging items.<br /><br />The overall atmosphere suggests that this may be a modern office space or <br />co-working area where people can come to work, relax, or socialize.</span></pre><h4 name="b485" id="b485" class="graf graf--h4 graf-after--pre">Explaining Memes</h4><p name="19e4" id="19e4" class="graf graf--p graf-after--h4">Objectively describing a scene is simpler than understanding and explaining humor. Let‚Äôs see how the model explains the meme below.</p><figure name="7932" id="7932" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IqUoZEX2CYOsX6oFIVeuIw.jpeg" data-width="800" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*IqUoZEX2CYOsX6oFIVeuIw.jpeg"><figcaption class="imageCaption">Building with AI meme. Image by¬†author.</figcaption></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="94ee" id="94ee" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># ask model to explain meme</span><br />stream = ollama.chat(<br />    model=<span class="hljs-string">&#x27;llama3.2-vision&#x27;</span>,<br />    messages=[{<br />        <span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>,<br />        <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Can you explain this meme to me?&#x27;</span>,<br />        <span class="hljs-string">&#x27;images&#x27;</span>: [<span class="hljs-string">&#x27;images/ai-meme.jpeg&#x27;</span>]<br />    }],<br />    stream=<span class="hljs-literal">True</span>,<br />)<br /><br /><span class="hljs-comment"># print stream</span><br /><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> stream:<br />    <span class="hljs-built_in">print</span>(chunk[<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>], end=<span class="hljs-string">&#x27;&#x27;</span>, flush=<span class="hljs-literal">True</span>)</span></pre><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="plaintext" name="81fa" id="81fa" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">The meme depicts Patrick Star from SpongeBob SquarePants, surrounded by <br />various AI tools and symbols. The caption reads &quot;Trying to build with AI <br />today...&quot; The image humorously illustrates the challenges of using AI in <br />building projects, implying that it can be overwhelming and frustrating.</span></pre><p name="1acb" id="1acb" class="graf graf--p graf-after--pre">The model does a good job here. It understands that the image is funny while also conveying the pain that people face.</p><h4 name="a848" id="a848" class="graf graf--h4 graf-after--p">OCR</h4><p name="1481" id="1481" class="graf graf--p graf-after--h4">The last use case is optical character recognition (OCR). This involves extracting text from images, which is valuable in a wide range of contexts. Here, I‚Äôll see if the model can translate a screenshot from my notes app to a markdown file.</p><figure name="e11a" id="e11a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PRSGngwjIVW01cLHK41lNg.jpeg" data-width="800" data-height="1308" src="https://cdn-images-1.medium.com/max/800/1*PRSGngwjIVW01cLHK41lNg.jpeg"><figcaption class="imageCaption">Screenshot of <a href="https://towardsdatascience.com/5-ai-projects-you-can-build-this-weekend-with-python-c57724e9c461" data-href="https://towardsdatascience.com/5-ai-projects-you-can-build-this-weekend-with-python-c57724e9c461" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">5 AI project ideas</a>. Image by¬†author.</figcaption></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="fa25" id="fa25" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content"><span class="hljs-comment"># ask model to read screenshot and convert to markdown</span><br />stream = ollama.chat(<br />    model=<span class="hljs-string">&#x27;llama3.2-vision&#x27;</span>,<br />    messages=[{<br />        <span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>,<br />        <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Can you transcribe the text from this screenshot in a \<br />                    markdown format?&#x27;</span>,<br />        <span class="hljs-string">&#x27;images&#x27;</span>: [<span class="hljs-string">&#x27;images/5-ai-projects.jpeg&#x27;</span>]<br />    }],<br />    stream=<span class="hljs-literal">True</span>,<br />)<br /><br /><span class="hljs-comment"># read stream</span><br /><span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> stream:<br />    <span class="hljs-built_in">print</span>(chunk[<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>], end=<span class="hljs-string">&#x27;&#x27;</span>, flush=<span class="hljs-literal">True</span>)</span></pre><pre data-code-block-mode="0" spellcheck="false" name="a041" id="a041" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">Here is the transcription of the text in markdown format:<br><br>5 AI Projects You Can Build This Weekend (with Python)<br><br>1. **Resume Optimization (Beginner)**<br> * Idea: build a tool that adapts your resume for a specific job description<br>2. **YouTube Lecture Summarizer (Beginner)**<br> * Idea: build a tool that takes YouTube video links and summarizes them<br>3. **Automatically Organizing PDFs (Intermediate)**<br> * Idea: build a tool to analyze the contents of each PDF and organize them <br>into folders based on topics<br>4. **Multimodal Search (Intermediate)**<br> * Idea: use multimodal embeddings to represent user queries, text knowledge, <br>and images in a single space<br>5. **Desktop QA (Advanced)**<br> * Idea: connect a multimodal knowledge base to a multimodal model like <br>Llama-3.2-11B-Vision<br><br>Note that I&#39;ve added some minor formatting changes to make the text more <br>readable in markdown format. Let me know if you have any further requests.</span></pre><p name="a078" id="a078" class="graf graf--p graf-after--pre">Again, the model does a decent job out of the box. While it missed the header, it accurately captured the content and formatting of the project ideas.</p><div name="a37e" id="a37e" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/multimodal-ai/1-mm-llms at main ¬∑ ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/multimodal-ai/1-mm-llms at main ¬∑‚Ä¶</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/1-mm-llms" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="01a1bd9ecfd4f408ff21fda180fe8df0" data-thumbnail-img-id="0*OH8QcDWZlcNZz-dA" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*OH8QcDWZlcNZz-dA);"></a></div><h3 name="4b17" id="4b17" class="graf graf--h3 graf-after--mixtapeEmbed">What‚Äôs next?</h3><p name="dea6" id="dea6" class="graf graf--p graf-after--h3">Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs).</p><p name="1c34" id="1c34" class="graf graf--p graf-after--p">However, there are other types of multimodal models. In the <a href="https://towardsdatascience.com/multimodal-embeddings-an-introduction-5dc36975966f" data-href="https://towardsdatascience.com/multimodal-embeddings-an-introduction-5dc36975966f" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">next article</a> of this series, I will discuss multimodal embedding models, which encode multiple data modalities (e.g. text and images) into a shared representation space.</p><p name="0fac" id="0fac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">More on Multimodal models üëá</strong></p><div name="847c" id="847c" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/fe9521d0e77a" data-href="https://shawhin.medium.com/list/fe9521d0e77a" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/fe9521d0e77a"><strong class="markup--strong markup--mixtapeEmbed-strong">Multimodal AI</strong><br>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/fe9521d0e77a" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="6105bb036ca97dea6117e7ac234d7d0a" data-thumbnail-img-id="0*d9f0f35363d134d5ddb508e140d32649a2f1ca00.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*d9f0f35363d134d5ddb508e140d32649a2f1ca00.jpeg);"></a></div></div></div></section><section name="d478" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2576" id="2576" class="graf graf--p graf--leading">üëâ <strong class="markup--strong markup--p-strong">Get FREE access to every new story I write </strong><a href="https://shawhin.medium.com/subscribe" data-href="https://shawhin.medium.com/subscribe" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">(Learn More)</strong></a></p><p name="8ba2" id="8ba2" class="graf graf--p graf-after--p">[1] <a href="https://arxiv.org/abs/1705.09406" data-href="https://arxiv.org/abs/1705.09406" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Multimodal Machine Learning: A Survey and Taxonomy</a></p><p name="cdd0" id="cdd0" class="graf graf--p graf-after--p">[2] <a href="https://arxiv.org/abs/2306.13549" data-href="https://arxiv.org/abs/2306.13549" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A Survey on Multimodal Large Language Models</a></p><p name="36d1" id="36d1" class="graf graf--p graf-after--p">[3] <a href="https://arxiv.org/abs/2304.08485" data-href="https://arxiv.org/abs/2304.08485" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Visual Instruction Tuning</a></p><p name="01cb" id="01cb" class="graf graf--p graf-after--p">[4] <a href="https://arxiv.org/abs/2410.21276" data-href="https://arxiv.org/abs/2410.21276" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GPT-4o System Card</a></p><p name="2f50" id="2f50" class="graf graf--p graf-after--p">[5] <a href="https://arxiv.org/abs/2410.13848" data-href="https://arxiv.org/abs/2410.13848" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</a></p><p name="40fa" id="40fa" class="graf graf--p graf-after--p">[6] <a href="https://arxiv.org/abs/2103.00020" data-href="https://arxiv.org/abs/2103.00020" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Learning Transferable Visual Models From Natural Language Supervision</a></p><p name="16bd" id="16bd" class="graf graf--p graf-after--p">[7] <a href="https://arxiv.org/abs/2204.14198" data-href="https://arxiv.org/abs/2204.14198" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Flamingo: a Visual Language Model for Few-Shot Learning</a></p><p name="c0cc" id="c0cc" class="graf graf--p graf-after--p">[8] <a href="https://arxiv.org/abs/2410.11190" data-href="https://arxiv.org/abs/2410.11190" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</a></p><p name="be5c" id="be5c" class="graf graf--p graf-after--p">[9] <a href="https://arxiv.org/abs/2409.18869" data-href="https://arxiv.org/abs/2409.18869" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Emu3: Next-Token Prediction is All You Need</a></p><p name="e5f1" id="e5f1" class="graf graf--p graf-after--p graf--trailing">[10] <a href="https://arxiv.org/abs/2405.09818" data-href="https://arxiv.org/abs/2405.09818" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Chameleon: Mixed-Modal Early-Fusion Foundation Models</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/5c6737c981d3"><time class="dt-published" datetime="2024-11-19T11:01:59.650Z">November 19, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>